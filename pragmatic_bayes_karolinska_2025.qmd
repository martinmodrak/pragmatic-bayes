---
title: "Pragmatic Bayes"
format: 
  revealjs:
    theme: [default, custom.scss]
    incremental: true
author: Martin Modr√°k
date: 2025-05-13
---

# What is this about? {background-color="black" background-image=img/Night-sky-milky-way-galaxy-astrophotography_-_West_Virginia_-_ForestWander.jpg}

:::{.image-source}
Modified from <a href="https://commons.wikimedia.org/wiki/File:Night-sky-milky-way-galaxy-astrophotography_-_West_Virginia_-_ForestWander.jpg">http://www.ForestWander.com</a>, <a href="https://creativecommons.org/licenses/by-sa/3.0/us/deed.en">CC BY-SA 3.0 US</a>, via Wikimedia Commons,
https://mc-stan.org/bayesplot/
:::


## I don‚Äôt want to convert you {background-color="black" background-image=img/convert_bayes.jpg}

:::{.image-source}
<a href="https://commons.wikimedia.org/wiki/File:Jehovah%27s_Witnesses_outside_the_British_Museum_02.jpg">Philafrenzy</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>, via Wikimedia Commons,
<a href="https://commons.wikimedia.org/wiki/File:Thomas_Bayes.gif">[2][3]</a>, Public domain, via Wikimedia Commons,
<a href="https://commons.wikimedia.org/wiki/File:Bayes_icon.jpg">Gnathan87</a>, CC0, via Wikimedia Commons
:::

## Pragmatic Bayes: a collection of ideas

:::{.fragment}
### Contrast with full _Bayesian epistemology_
:::
:::{.fragment}
No shaming our subjective/objective Bayesian friends!
:::
:::{.fragment}

Epistemology / philsci is not solved
:::
:::{.fragment}

### Can we do Bayesian [_statistics_]{.highlight1} without committing to Bayesian [_philosophy_]{.highlight2}?
:::

:::{.notes}
You can like some and reject other ideas
:::

## Main Influences

Deborah Mayo

Andrew Gelman

Stan community more broadly

Berna Devezer

Danielle J. Navarro

:::{.notes}
Mayo : severe testing

Gelman : Theoretical stastic is the theory of applied statistics

Stan : solve problems

Devezer : pluralism, rigour

Navarro : hypothesis testing
:::

## Outline


# Theoretical background {background-image=img/Triang-cyl-sph4.svg.png background-color="black"}

:::{.image-source}
<a href="https://commons.wikimedia.org/wiki/File:Triang-cyl-sph4.svg">Ag2gaeh</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>, via Wikimedia Commons
:::

## Frequentist calibration

#### (continuous parameter)

- Confidence interval: 
  - For [any fixed]{.highlight1} parameter value, $x\%$ CI contains the true value [at least]{.highlight2} $x\%$ of the time.
  - Worst case
  - Usually conservative approximation

:::{.notes}
Replacing the inequality with strict equality would make the interval not exist in many cases.

Generalizes to confidence sets.
:::

## Bayesian calibration

#### (continuous parameter)

- Credible interval
  - [Averaged]{.highlight1} over the prior, $x\%$ CrI contains the true value [exactly]{.highlight2} $x\%$ of the time.
  - Specific values may lead to low coverage
  - Usually exact*

:::{.notes}
Typically MCMC, so not "exact computation", but we get an imprecise value for the "exact bound"

For discrete parameters need to convert to probability statements
:::

## Calibration example

<!-- TODO binomial proportion flat prior vs. Clopper-Pearson -->

## Other inference goals

### Frequentist Test = inverted confidence interval

:::{.fragment}
### Bayes factor = calibrated posterior model probability
:::

:::{.notes}
We can more or less convert every uncertainty statement into confidence/credible interval

squared error, bias not here


:::

# Pragmatic Bayesian can outfreq the freq {background-image=img/tail_probs.jpg}

:::{.image-source}
Modified from <a href="https://commons.wikimedia.org/wiki/File:Null-hypothesis-region-eng.png">Smahdavi4</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>, via Wikimedia Commons
:::

## Bayes approximates freq

:::{.fragment}
### [Bernstein-von Mises](https://en.wikipedia.org/wiki/Bernstein%E2%80%93von_Mises_theorem)
:::
:::{.fragment}

We do not live in asymptotic utopia
:::

## Most freq methods are approximations!

- ML + Normal approximation
- ML + profile likelihood 
  - $\chi^2$ asymptotics of the likelihood-ratio test
  - Computationally expensive! <!-- (TODO test confint for complex lmer vs. brms) -->

:::{.notes}
Prove that CLT holds

They are valid only in the asymptotic utopia

For profile likelihood, finding a single confidence bound is slightly more costly than fitting the model

Bonus for penalized: problems at boundary values
:::

## Bayes as a freq tool

<!-- TODO brms vs glm.nb / mgcv / gamlss -->

:::{.notes}
‚ÄúThe frequentist theory of Bayesian statistics‚Äù https://staff.science.uva.nl/b.j.k.kleijn/bkleijn-book-work-in-progress-Sep2022.pdf
https://www.jstor.org/stable/pdf/27028770.pdf (dense paper version)
:::

## It is hard to be a frequentist! 

### (with [exact]{.highlight1} [finite-sample]{.highlight2} guarantees)

- Exact freq computation is _very hard_
- Freq properties hard to empirically check
- Bayesian computation can be verified empirically ([SBC](https://doi.org/10.1214/23-BA1404), [Yao et al.](https://arxiv.org/abs/2305.14593))

:::{.notes}
Freq. properties cannot be exhaustively checked empirically
Bayesian computation can be (SBC + Yao approach)

If we both do Laplace approximations, is there any difference?
:::

# Pragmatic Bayesian tests their assumptions {background-color="black"}

## Which assumptions?

### All of them

- Prior
- Likelihood
- Computation
- [Bayesian workflow](https://arxiv.org/abs/2011.01808)

:::{.notes}
Defensible prior often easy to find

One coherent approach!

Assumptions can be tested also in freq, but less obviously.
:::

## Modern Bayesian computation succeeds or fails loudly {.single-point}

:::{.notes}
Note how the freq NB GLMs fail, but give no warning
:::


# Pragmatic Bayesian understands Bayesian limits {background-color="black"}

## Selection effects

10 patients get sick, 10 recover. What is the likely recovery probability?

:::{.fragment}
Should your inference change if you learn that the experimenter would only ever report the results to you if everybody recovered?
:::

:::{.fragment}
In Bayesian statistics it should not! ü§Ø

$$
\pi_\text{post}(\theta \mid y, \text{ accept(y)}) = \pi_\text{post}(\theta \mid y)
$$
:::

:::{.fragment}
The [likelihood principle]() <!--TODO link -->
:::

## Selection effects - intuition

Frequentist simulation: 

```
prob_recovery = some_number
repeat {
  y = binomial_rng(N = 10, prob = prob_recovery)
  if(y == 10) then break
}
```

:::{.fragment}
Bayesian simulation:

```
repeat {
  prob_recovery = prior_rng()
  y = binomial_rng(N = 10, prob = prob_recovery)
  if(y == 10) then break
}
```

:::

:::{.notes}
The Bayesian answer makes sense under repeated draws from the prior!

In practice there is probably a mix of both types of selection (repeating with fixed value, getting a "new draw from the prior").
:::

## Modelling selection

Bayesian can model the frequentist process as a truncated distribution.

:::{.fragment}
In the example, this leads to $\pi_\text{post}(\theta \mid y) = \pi_\text{prior}(\theta)$.

<!-- TOOD link Betancourt -->
More generally to an interesting [class of models]()
:::

:::{.fragment}
Pragmatically: we rarely can build a good model of the selection
:::

:::{.notes}
Example - we want to infer timing of bloom from historical specimens with known date of collection, but only flowering plants are ever collected.

:::

## Early stopping

Should your inferences change when you learn that data collection stopped once $p < 0.05$?

- The likelihood principle says NO
- No easy cop-out this time

## Early stopping - example

TODO test coverage

## Possible solutions to early stopping

- Smarter stopping rules
- Include time in your model

## Frequentist don't have it all
- Freq approaches to early stopping approximate
  - And bespoke/limited 
- You need simulations anyway

:::{.notes}

Limited: no 3 phase group seq + two recalculations

Guidelines explicitly require a ton of simulations if you are proposing a non-standard method (TODO dig the reference)
:::

# Pragmatic Bayesian is not afraid of freq {background-color="black"}

## Some use cases of freq

- Sequential designs
- Freq as approximate Bayes
  - Flat prior + normal approximation
  - Can [check with SBC](https://hyunjimoon.github.io/SBC/articles/implementing_backends.html)

# Pragmatic Bayesian thinks about causality {background-color="black"}

McElreath ‚Äúfull luxury‚Äù? Bayes TODO
Causal inference as decision theory (TODO)

# Pragmatic Bayesian aligns inferences with scientific questions {background-color="black"}

## Rich models 

- Varying scale between groups already difficult in freq

- Differential equations

- hidden Markov models 


:::{.notes}
HMMs: Hype Vianey
:::

## Rich inferences

Inferences on derived quantities, prediction intervals?

- Just compute per sample! 

- Some questions don‚Äôt have freq answers without regularization (e.g. mean of general distribution). 
  - Once you regularize, you may as well do Bayes.

:::{.notes}
Inference on linear combinations of parameters is already annoying in freq.

Building freq prediction intervals with finite-sample guarantees is hard!
:::

# My personal non-reasons to use Bayes {background-color="black"}


:::{.notes}
Philosophical arguments that I find not very practically useful.
:::

## Prior information 

No!

:::{.fragment}
We can rarely elicit and express precise enough priors
:::

## Sequential updating? 

No!

:::{.fragment}
In practice just fit a big model to all data.
:::


## Epistemology? 

No!

:::{.fragment}
Severity of tests matters.
:::

:::{.fragment}
Especially no to Bayes factors.
:::

# Why I use Bayesian tools {background-color="black"}

And maybe you should too?

## Pragmatic reasons

- Useful!
- One coherent approach!
- Finite sample guarantees!
- Rich models!

### Thanks for your attention!
