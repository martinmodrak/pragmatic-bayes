---
title: "Pragmatic Bayes"
format: 
  revealjs:
    theme: [default, custom.scss]
    incremental: true
    slide-number: true
    output-file: index.html
author: Martin Modr√°k
date: 2025-05-13
---

<!--
Notes to improve next time:
Frequentist calibration example - it has Bayesian methods, but shows freq calibration
"one coherent approach" not emphasized enough
Didn't really mention on philosophical commitments to nature of probability
More emphasize that I want to solve problems - typically estimation problems
-->

```{r setup, include=FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
theme_set(theme_minimal(base_size = 20))
cache_dir <- "sim_cache"
```


# What is this about? {background-color="black" background-image=img/Night-sky-milky-way-galaxy-astrophotography_-_West_Virginia_-_ForestWander.jpg}

:::{.image-source}
Modified from <a href="https://commons.wikimedia.org/wiki/File:Night-sky-milky-way-galaxy-astrophotography_-_West_Virginia_-_ForestWander.jpg">http://www.ForestWander.com</a>, <a href="https://creativecommons.org/licenses/by-sa/3.0/us/deed.en">CC BY-SA 3.0 US</a>, via Wikimedia Commons,
https://mc-stan.org/bayesplot/
:::


## I don‚Äôt want to convert you {background-color="black" background-image=img/convert_bayes.jpg}

:::{.image-source}
<a href="https://commons.wikimedia.org/wiki/File:Jehovah%27s_Witnesses_outside_the_British_Museum_02.jpg">Philafrenzy</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>, via Wikimedia Commons,
<a href="https://commons.wikimedia.org/wiki/File:Thomas_Bayes.gif">[2][3]</a>, Public domain, via Wikimedia Commons,
<a href="https://commons.wikimedia.org/wiki/File:Bayes_icon.jpg">Gnathan87</a>, CC0, via Wikimedia Commons
:::

## Pragmatic Bayes: a collection of ideas

:::{.fragment}
### Contrast with full _Bayesian epistemology_
:::
:::{.fragment}
No shaming our subjective/objective Bayesian friends!
:::
:::{.fragment}

Epistemology / philsci is not solved
:::
:::{.fragment}

### Can we do Bayesian [_statistics_]{.highlight1} without committing to Bayesian [_philosophy_]{.highlight2}?
:::

:::{.notes}
You can like some and reject other ideas
:::

## Main Influences

Deborah Mayo

Andrew Gelman

Stan community more broadly

Berna Devezer

Danielle J. Navarro

:::{.notes}
Mayo : severe testing

Gelman : Theoretical stastic is the theory of applied statistics

Stan : solve problems

Devezer : pluralism, rigour

Navarro : hypothesis testing

And many others
:::


# Theoretical background {background-image=img/Triang-cyl-sph4.svg.png background-color="black"}

:::{.image-source}
<a href="https://commons.wikimedia.org/wiki/File:Triang-cyl-sph4.svg">Ag2gaeh</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>, via Wikimedia Commons
:::

## Frequentist calibration

#### (continuous parameter)

- Confidence interval: 
  - For [any fixed]{.highlight1} parameter value, $x\%$ CI contains the true value [at least]{.highlight2} $x\%$ of the time.
  - Worst case
  - Usually conservative approximation

:::{.notes}
Replacing the inequality with strict equality would make the interval not exist in many cases.

Generalizes to confidence sets.
:::

## Bayesian calibration

#### (continuous parameter)

- Credible interval
  - [Averaged]{.highlight1} over the prior, $x\%$ CrI contains the true value [exactly]{.highlight2} $x\%$ of the time.
  - Specific values may lead to low coverage
  - Usually exact*

:::{.notes}
Typically MCMC, so not "exact computation", but we get an imprecise value for the "exact bound"

For discrete parameters need to convert to probability statements
:::

## Frequentist calibration example

$$
y \sim \text{Binomial}(20, \theta)
$$

:::{.fragment}
```{r binomial-prop}
source("binomial_proportion_coverage.R")
```
:::

## Other inference goals

### Frequentist Test = inverted confidence interval

:::{.fragment}
### Bayes factor = calibrated posterior model probability
:::

:::{.notes}
We can more or less convert every uncertainty statement into confidence/credible interval

squared error, bias not here


:::

# Pragmatic Bayesian can outfreq the freq {background-image=img/tail_probs.jpg}

:::{.image-source}
Modified from <a href="https://commons.wikimedia.org/wiki/File:Null-hypothesis-region-eng.png">Smahdavi4</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>, via Wikimedia Commons
:::

## Bayes approximates freq

:::{.fragment}
### [Bernstein-von Mises](https://en.wikipedia.org/wiki/Bernstein%E2%80%93von_Mises_theorem)
:::
:::{.fragment}

We do not live in asymptotic utopia
:::

## Most freq methods are approximations!

- ML + Normal approximation
- ML + profile likelihood 
  - $\chi^2$ asymptotics of the likelihood-ratio test
  - Computationally expensive! 
  
:::{.notes}
Prove that CLT holds

They are valid only in the asymptotic utopia

For profile likelihood, finding a single confidence bound is slightly more costly than fitting the model

Bonus for penalized: problems at boundary values
:::

## Bayes as a freq tool - example

Fitting a negative binomial model, with 4 observations per group:

Frequentist via `MASS` package:

```r
MASS::glm.nb(y ~ group, data = data)
```

Frequentist via `gamlss` package:

```r
gamlss::gamlss(y ~ group, family = "NBI")
```


Bayesian with flat prior via `brms` package:

```r
brms::brm(y ~ group, data = data, family = "negbinomial", 
  prior = brms::prior("", class = "Intercept"))
```

## Bayes as a freq tool - example II

```{r bayes-freq}
coverage_cache_file <- file.path(cache_dir, "nb_coverage.rds")
if(!file.exists(coverage_cache_file)) {
  source("nb_coverage.R")
} 

nb_coverage_df <- readRDS(coverage_cache_file)

nb_coverage_df |> 
  filter(!is.na(ci_low), !is.na(ci_high)) |> 
  mutate(covered = b >= ci_low & b <= ci_high) |> 
  group_by(method, b, phi) |> 
  summarise(coverage = mean(covered),
            n_covered = sum(covered),
            coverage_low = qbeta(0.025, n_covered, n() - n_covered + 1),
            coverage_high = qbeta(0.975, n_covered + 1 , n() - n_covered ),
            .groups = "drop"
  ) |> 
  ggplot()  + aes(x = b, ymin = coverage_low, y = coverage, ymax = coverage_high) +
  geom_hline(color = "orangered", yintercept = 0.95) +
  geom_ribbon(fill = "#888", alpha = 0.3) +
  geom_line() + facet_grid(method~phi, labeller = "label_both") + 
  scale_x_continuous("True fold change") + theme(strip.text.y = element_text(size = 10))
```


:::{.notes}
‚ÄúThe frequentist theory of Bayesian statistics‚Äù https://staff.science.uva.nl/b.j.k.kleijn/bkleijn-book-work-in-progress-Sep2022.pdf
https://www.jstor.org/stable/pdf/27028770.pdf (dense paper version)
:::

## It is hard to be a frequentist! 

### (with [exact]{.highlight1} [finite-sample]{.highlight2} guarantees)

- Exact freq computation is _very hard_
- Freq properties hard to empirically check
- Bayesian computation can be verified empirically ([SBC](https://doi.org/10.1214/23-BA1404), [Yao et al.](https://arxiv.org/abs/2305.14593))

:::{.notes}
Freq. properties cannot be exhaustively checked empirically
Bayesian computation can be (SBC + Yao approach)

If we both do Laplace approximations, is there any difference?

checking lme4 results with rstanarm
:::

# Pragmatic Bayesian tests their assumptions {background-color="black" background-image=img/Test.jpeg}

:::{.image-source}
<a href="https://commons.wikimedia.org/wiki/File:Test_(student_assessment).jpeg">KF</a>, Public domain, via Wikimedia Commons
:::

## Which assumptions?

### All of them

- Prior
- Likelihood
- Computation
- [Bayesian workflow](https://arxiv.org/abs/2011.01808)

:::{.notes}
Defensible prior often easy to find

One coherent approach!

Assumptions can be tested also in freq, but less obviously.
:::

## Modern Bayesian computation succeeds or fails loudly {.single-point}

:::{.notes}
Note how the freq NB GLMs fail, but give no warning
:::


# Pragmatic Bayesian understands Bayesian limits {background-color="black" background-image=img/Safety_Signs.JPG}

:::{.image-source}
<a href="https://commons.wikimedia.org/wiki/File:Safety_Signs.JPG">TeWeBs</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>, via Wikimedia Commons
:::

## Selection effects

10 patients get sick, 10 recover. What is the likely recovery probability?

:::{.fragment}
Should your inference change if you learn that the experimenter would only ever report the results to you if everybody recovered?
:::

:::{.fragment}
In Bayesian statistics it should not! ü§Ø

$$
\pi_\text{post}(\theta \mid y, \text{ accept(y)}) = \pi_\text{post}(\theta \mid y)
$$
:::

:::{.fragment}
The [likelihood principle](https://en.wikipedia.org/wiki/Likelihood_principle)
:::

## Selection effects - intuition

Frequentist simulation: 

```
prob_recovery = some_number
repeat {
  y = binomial_rng(N = 10, prob = prob_recovery)
  if(y == 10) then break
}
```

:::{.fragment}
Bayesian simulation:

```
repeat {
  prob_recovery = prior_rng()
  y = binomial_rng(N = 10, prob = prob_recovery)
  if(y == 10) then break
}
```

:::

:::{.notes}
The Bayesian answer makes sense under repeated draws from the prior!

In practice there is probably a mix of both types of selection (repeating with fixed value, getting a "new draw from the prior").
:::

## Modelling selection

Bayesian can model the frequentist process as a truncated distribution.

:::{.fragment}
In the example, this leads to $\pi_\text{post}(\theta \mid y) = \pi_\text{prior}(\theta)$.

More generally to an interesting [class of models]()
:::

:::{.fragment}
Pragmatically: we rarely can build a good model of the selection
:::

:::{.notes}
Example - we want to infer timing of bloom from historical specimens with known date of collection, but only flowering plants are ever collected.

Medical: modelling people who show up at the doctors office

:::

## Early stopping

Should your inferences change when you learn that data collection stopped once $p < 0.05$?

- The likelihood principle says NO
- No easy cop-out this time

## Early stopping - example

Binomial model, max steps = 10000, optional stopping after each step

:::: {.columns}

::: {.column width="33%"}

:::{.fragment}

Stopping when 95% CrI excludes 0.5

```{r early-stop}
#| fig-width: 3.5
#| fig-height: 3.5
source("binomial_early_stop.R")
lims <- expand_limits(y = c(0.35,1))
plot_coverage_stop(coverage_stop_05) + lims
```
:::

:::

::: {.column width="33%"}

:::{.fragment}

Stopping when 95% CrI excludes 0.4 - 0.6


```{r}
#| fig-width: 3.5
#| fig-height: 3.5
plot_coverage_stop(coverage_stop_04_06) + lims
```
:::

:::

::: {.column width="33%"}


:::{.fragment}

Stopping when the width of 95% CrI is < 0.1

```{r}
#| fig-width: 3.5
#| fig-height: 3.5
plot_coverage_stop(coverage_stop_width_01) + lims
```
:::

:::

::::


## Possible solutions to early stopping

- Smarter stopping rules
- Include time in your model
- Simulate to get freq properties
  - Shoutout to bayesflow

## Frequentist don't have it all
- Freq approaches to early stopping approximate
  - And bespoke/limited 
- You need simulations anyway

:::{.notes}

Limited: no 3 phase group seq + two recalculations

Regulators explicitly require a ton of simulations if you are proposing a non-standard method
:::

# Pragmatic Bayesian is not afraid of freq {background-color="black" background-image=img/Beyond_the_Dark_Angel_-_metal_band_photo.jpg}
:::{.image-source}
<a href="https://commons.wikimedia.org/wiki/File:Beyond_the_Dark_Angel_-_metal_band_photo.jpg">Luk√°≈° Beneda</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>, via Wikimedia Commons
:::

## Some use cases of freq

- Sequential designs
- Freq as approximate Bayes
  - Flat prior + normal approximation
  - Can [check with SBC](https://hyunjimoon.github.io/SBC/articles/implementing_backends.html)

# Pragmatic Bayesian thinks about causality {background-color="black" background-image=img/Causal_assumptions_for_developing_bias-minimized_models_for_markers_of_imminent_myocardial_infarction.webp}

:::{.image-source}
<a href="https://commons.wikimedia.org/wiki/File:Causal_assumptions_for_developing_bias-minimized_models_for_markers_of_imminent_myocardial_infarction.webp">Authors of the study: Stefan Gustafsson, Erik Lampa, Karin Jensevik Eriksson, Adam S. Butterworth, S√∂lve Elmst√•hl, Gunnar Engstr√∂m, Kristian Hveem, Mattias Johansson, Arnulf Langhammer, Lars Lind, Kristi L√§ll, Giovanna Masala, Andres Metspalu, Conchi Moreno-Iribas, Peter M. Nilsson, Markus Perola, Birgit Simell, Hemmo Sipsma, Bj√∏rn Olav √Ösvold, Erik Ingelsson, Ulf Hammar, Andrea Ganna, Bodil Svennblad, Tove Fall &amp; Johan Sundstr√∂m</a>, <a href="https://creativecommons.org/licenses/by/4.0">CC BY 4.0</a>, via Wikimedia Commons
:::

## Bayesian causality

- McElreath: [Full luxury Bayesian  inference](https://elevanth.org/blog/2021/06/29/regression-fire-and-dangerous-things-3-3/)

- Dawid: [Causal inference as decision theory](http://dx.doi.org/10.1515/jci-2020-0008)

- Inverse probability of treatment weighting is the selection framework we discussed

:::{.notes}
We can jointly estimate IPTW and the main model
:::

# Pragmatic Bayesian aligns inferences with scientific questions {background-color="black" background-image=img/Custom_Made_Globes.jpg}

:::{.image-source}
<a href="https://commons.wikimedia.org/wiki/File:Cartographic_Publishing_-_Custom_Made_Globes_(NBY_5792).jpg">Bergin, Jerry</a>, Public domain, via Wikimedia Commons
:::

## Rich models 

- Varying scale between groups already difficult in freq

- Differential equations

- hidden Markov models

- Simulation-based inference (Bayesflow workshop yesterday)

:::{.notes}
HMMs: Hype Vianey
:::

## Rich inferences

Inferences on derived quantities, prediction intervals?

- Just compute per sample! 

- Some questions don‚Äôt have freq answers without regularization (e.g. mean of general distribution). 
  - Once you regularize, you may as well do Bayes.

:::{.notes}
Inference on linear combinations of parameters is already annoying in freq.

Building freq prediction intervals with finite-sample guarantees is hard!
:::

# My personal non-reasons to use Bayes {background-color="black" background-image=img/West_Africa.jpg}

:::{.image-source}
<a href="https://commons.wikimedia.org/wiki/File:West_Africa_(2175061620).jpg">Steve Evans from Citizen of the World</a>, <a href="https://creativecommons.org/licenses/by/2.0">CC BY 2.0</a>, via Wikimedia Commons
:::

:::{.notes}
Philosophical arguments that I find not very practically useful.
:::

## Prior information 

No!

:::{.fragment}
We can rarely elicit and express precise enough priors
:::

## Sequential updating? 

No!

:::{.fragment}
In practice just fit a big model to all data.
:::


## Epistemology? 

No!

:::{.fragment}
Severity of tests matters.
:::

:::{.fragment}
Especially no to Bayes factors.
:::

# Why I use Bayesian tools {.lower background-color="black" background-image=img/EXCITE_2024.jpg} 

And maybe you should too?

:::{.image-source}
<a href="https://commons.wikimedia.org/wiki/File:EXCITE_2024-_Launch_and_Recovery_(SVS14726_-_5_-_EXCITE_Balloon_Inflates).jpg">NASA&#039;s Scientific Visualization Studio - Advocates in Manpower Management, Inc./Sophia Roberts, University of Maryland College Park/Jeanette Kazmierczak</a>, Public domain, via Wikimedia Commons
:::

## Pragmatic reasons

- Useful!
- One coherent approach!
- Finite sample guarantees!
- Rich models!

:::{.fragment}
### Thanks for your attention!

Slide & simulation sources:   
[https://github.com/martinmodrak/pragmatic-bayes](https://github.com/martinmodrak/pragmatic-bayes)

:::{.qrcode}
```{r qr, fig.width=3,fig.height=3}
plot(qrcode::qr_code("https://github.com/martinmodrak/pragmatic-bayes"))
```

:::

:::

